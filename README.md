# Agentic AI Chatbot for **USMLEÂ Stepâ€‘1**



## 1Â Â Overview

This project is an endâ€‘toâ€‘end, **retrievalâ€‘augmentedâ€‘generation (RAG)** system that answers United States Medical Licensing Examinationâ€¯(USMLE) Stepâ€‘1 questions. It couples a custom medical knowledge base stored in Pinecone with an agentic reasoning graph built with LangGraph. The chatbot is exposed through a lightweight **Flask** API and an interactive **Streamlit** frontâ€‘end, and is fully containerised with separate Docker images for the backâ€‘end and frontâ€‘end plus a `dockerâ€‘compose` orchestrator.



## 2Â Â Motivation

Studying for Stepâ€‘1 requires rapid recall of thousands of facts spread across diverse sources. Traditional flashâ€‘card or static Qâ€‘bank apps cannot personalise explanations in real time. The goal of this project is to provide an **onâ€‘demand tutor** that can:

* surface the most relevant authoritative content for any highâ€‘yield question;
* justify each answer with concise clinical reasoning; and
* improve continuously through automated tracing & evaluation.



## 3Â Â Goals & Objectives

1. **Highâ€‘precision retrieval.** Embed curated medical texts with `MedEmbedâ€‘largeâ€‘v0.1` and store them in Pinecone for millisecondâ€‘level semantic search.
2. **Agentic reasoning.** Orchestrate document retrieval, query rewriting, web search fallâ€‘back, answer generation, and grading through a LangGraph state machine.
3. **Observability out of the box.** Collect structured traces in **LangSmith** and score every response for faithfulness & relevancy via **JudgmentLabs (Judgeval)**.
4. **Seamless deployment.** Provide a oneâ€‘command `dockerâ€‘compose` stack that spins up the API and UI with sensible defaults.



## 4Â Â Solution Approach

### 4.1Â Â RAG Pipeline

```
User âœ Streamlit âœ Flask /chat âœ LangGraph Agent
          â”‚                    â”‚
          â”‚                    â”œâ”€â–¶ RetrieverTool (Pinecone)
          â”‚                    â”œâ”€â–¶ Queryâ€‘Rewriter (LLM)
          â”‚                    â”œâ”€â–¶ Tavily WebÂ Search
          â”‚                    â””â”€â–¶ Answer Generator (LLM)
          â–¼
        JSON Response âœ Streamlit renderer
```
<!-- Architecture diagram -->
<p align="center">
          <img width="520" alt="image" src="https://github.com/user-attachments/assets/dbc01165-83bb-4069-80ea-f07202ab81ef" />
</p>


1. **Embedding & StorageÂ :** All source documents are embedded offline with `MedEmbedâ€‘largeâ€‘v0.1` and pushed to a dedicated Pinecone index.
2. **Conversation FlowÂ :** The LangGraph graph routes each user message through specialised nodes. If retrieval fails a grader node triggers query rewriting or realâ€‘time web search.
3. **Answer FormatÂ :** The assistant returns a 2â€‘4Â sentence rationale followed by the single best answer on a new line (e.g. `**Answer:Â Tamsulosin**`).

### 4.2Â Â Tracing & Evaluation

* **LangSmith**: Every run records prompts, tool calls, latencies, and token usage. This enables offline inspection and prompt tuning.
* **JudgmentLabs**: Each response is automatically scored with *AnswerÂ Relevancy* and *Faithfulness* scorers (thresholdÂ â‰¥â€¯0.8). Scores and execution graphs are saved back to LangSmith for unified observability.

### 4.3Â Â Frontâ€‘end

The Streamlit UI mimics a chat interface, persists history with `shelve`, and calls the backâ€‘end REST endpoint. A custom CSS layer (`style/chatbot_style.py`) applies a dark mode theme and brand imagery.



## 5Â Â TechÂ Stack

| Layer           | Technology                                           | Purpose                              |
| --------------- | ---------------------------------------------------- | ------------------------------------ |
| LLMÂ /â€¯Reasoning | **OpenAIÂ GPTâ€‘4.1â€‘mini**                              | Primary language model               |
| Embeddings      | **MedEmbedâ€‘largeâ€‘v0.1 (HF)**                         | Domainâ€‘specific text embeddings      |
| Vector Store    | **Pinecone**                                         | Approximateâ€‘nearestâ€‘neighbour search |
| Orchestration   | **LangChainÂ +Â LangGraph**                            | Tool binding & state graph           |
| Evaluation      | **JudgmentLabsÂ (Judgeval)**                          | Automated scoring                    |
| Tracing         | **LangSmith**                                        | Endâ€‘toâ€‘end run tracing               |
| Backâ€‘end        | **FlaskÂ +Â uvicorn via uv**                           | REST API & health checks             |
| Frontâ€‘end       | **Streamlit**                                        | Chat UI                              |
| Infrastructure  | DockerÂ (backend & frontend images), `dockerâ€‘compose` | LocalÂ /Â edge deployment              |



## 6Â Â RepositoryÂ Layout

```
.
â”œâ”€ .streamlit/            # Streamlit config
â”œâ”€ artifacts/             # Persisted chat history & logs
â”œâ”€ notebook/              # Exploration / dataset prep
â”œâ”€ src/
â”‚   â”œâ”€ Agentic_RAG_Evaluation.py  # Core LangGraph workflow
â”‚   â”œâ”€ logger.py         # Structured logging helper
â”‚   â””â”€ custome_exception.py
â”œâ”€ utils/                 # Misc helpers
â”œâ”€ style/                 # CSS & images
â”œâ”€ app.py                 # Flask entryâ€‘point
â”œâ”€ streamlit_app.py       # Streamlit UI entryâ€‘point
â”œâ”€ Dockerfile.backend     # Build backend image
â”œâ”€ Dockerfile.frontend    # Build frontend image
â”œâ”€ dockerâ€‘compose.yml     # Orchestrate both services
â””â”€ requirements.txt / pyproject.toml
```



## 7Â Â GettingÂ Started

### 7.1Â Â Local Development (PythonÂ â‰¥â€¯3.11)

```bash
# Clone & enter
$ git clone https://github.com/Karthikmh2510/Agentic-AI-ChatBot-for-USMLE-Step1.git
$ cd Agentic-AI-ChatBot-for-USMLE-Step1

# Install deps (uv recommended)
$ pip install uv
$ uv pip install -r requirements.txt

# Environment
$ cp .env.example .env  # edit API keys & Pinecone index name

# Run services
$ python app.py          # starts backend on :8080
$ streamlit run streamlit_app.py  # starts UI on :8501
```

### 7.2Â Â ContainerisedÂ Deployment

1. **Build images** (optional â€“ Compose will build if theyâ€™re missing):

   ```bash
   docker build -f Dockerfile.backend -t usmle-rag-backend:latest .
   docker build -f Dockerfile.frontend -t usmle-rag-frontend:latest .
   ```
2. **Start the stack**:

   ```bash
   docker-compose up --build -d
   ```

   * Backâ€‘end: `http://localhost:8080` â€ƒ\* Frontâ€‘end: `http://localhost:8501`
3. **Shut down**:

   ```bash
   docker-compose down
   ```

### 7.3Â Â EnvironmentÂ Variables

Minimum keys required in `.env`:

```env
OPENAI_API_KEY=...
PINECONE_API_KEY=...
PINECONE_INDEX=
HUGGINGFACE_API_KEY=...
TAVILY_API_KEY=...
LANGSMITH_API_KEY=...
LANGSMITH_TRACING=true
LANGSMITH_ENDPOINT=https://api.smith.langchain.com
JUDGMENT_API_KEY=...
JUDGMENT_ORG_ID=...
```

## 8Â Â Usage

Open the Streamlit UI, type any Stepâ€‘1 style prompt (e.g. *â€œCompare Crohn disease with ulcerative colitis.â€*) and press Enter. The UI renders the dialogue with markdown support, tables, and inline LaTeX where relevant. Click **ClearÂ Chat** in the sidebar to reset the session.


## 9Â Â Observability & Quality Checks

* **Trace Viewer:** Each request appears in your \[LangSmith] dashboard with timeâ€‘stamped nodes and tokenâ€‘level costs.
* **Judgeval Reports:** After the answer node executes, two scorers evaluate relevancy & faithfulness using the retrieved context. Failing scores are logged for prompt refinement.



## 10Â Â Roadmap

* ğŸ”Â Add Perâ€‘question *explainâ€‘whyâ€‘score* feedback
* ğŸ¥Â Integrate UMLS &Â PubMed abstracts to widen knowledge base
* âš¡Â Serve models through GPUâ€‘enabled inference API (e.g. Groq or Together.ai) for faster answers
* ğŸ“ˆÂ Grafana / Prometheus metrics for latency & throughput monitoring



## 11Â Â License & Disclaimer

This work is released under the **MIT License**. All content is for **educational purposes only** and must **not** be used to make clinical decisions. Always consult a qualified physician for medical advice.
